{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Service Using Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No content found in response.\n",
      "WARNING:root:Retrying (1/3) after 10 seconds...\n",
      "INFO:root:Cost: 0.000215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我作為一個AI助手，並沒有情感，所以我無法說“好”或“不好”。不過，我一直保持運作良好，可以為您提供幫助。如果您需要任何幫助，請隨時告訴我。\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import requests\n",
    "from openai import OpenAIError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def _call_azure_api(prompt: str, system_prompt: str, temperature: float, max_tokens: int) -> str:\n",
    "    url = \"https://bigdata-openai-gpt-2.openai.azure.com/openai/deployments/bigdata-gpt35-2/chat/completions?api-version=2023-05-15\"\n",
    "    headers = {\"api-key\": \"3f5c8ab3de1545059600c578e3d96452\"}\n",
    "    \n",
    "    json_body = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    retries = 3\n",
    "    for retry in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=json_body)\n",
    "            response = response.json()\n",
    "            choices = response.get(\"choices\", [])\n",
    "            \n",
    "            if choices:\n",
    "                content = choices[0].get(\"message\", {}).get(\"content\")\n",
    "                if content:\n",
    "                    prompt_tokens = response[\"usage\"][\"prompt_tokens\"]\n",
    "                    completion_tokens = response[\"usage\"][\"completion_tokens\"]\n",
    "                    cost = _count_cost(\"gpt-3.5-turbo\", prompt_tokens, completion_tokens)\n",
    "                    logging.info(\"Cost: %f\", cost)\n",
    "                    return content\n",
    "                else:\n",
    "                    logging.error(\"No content found in response.\")\n",
    "            else:\n",
    "                logging.error(\"No choices found in response.\")\n",
    "            \n",
    "            if retry == retries - 1:\n",
    "                logging.error(\"Failed to generate a response after %d attempts. Aborting.\", retries)\n",
    "                raise\n",
    "            logging.warning(\"Retrying (%d/%d) after 10 seconds...\", retry + 1, retries)\n",
    "            time.sleep(10)\n",
    "        except OpenAIError as error:\n",
    "            logging.error(\"Error: %s\", error)\n",
    "            if retry == retries - 1:\n",
    "                logging.error(\"Failed to generate a response after %d attempts. Aborting.\", retries)\n",
    "                raise\n",
    "            logging.warning(\"Retrying (%d/%d) after 10 seconds...\", retry + 1, retries)\n",
    "            time.sleep(10)\n",
    "\n",
    "def _count_cost(model: str, prompt_tokens: float, completion_tokens: float) -> float:\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        return (prompt_tokens / 1000) * 0.0015 + (completion_tokens / 1000) * 0.002\n",
    "    return (prompt_tokens / 1000) * 0.003 + (completion_tokens / 1000) * 0.004\n",
    "\n",
    "res = _call_azure_api(\"妳好嗎\", \"妳要naughty的回答\", 0.8, 200)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改良GPT邏輯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一層 接收逐字稿 -> 計算最佳分配 -> 獲得第一次彙整的List => chunk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2600, 2190]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from typing import List\n",
    "import math\n",
    "def _count_tokens(content: str, model: str = \"gpt-3.5-turbo-0613\") -> int:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content,\n",
    "        },\n",
    "    ]\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "    }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4\n",
    "        tokens_per_name = -1\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "def read_txt_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except IOError:\n",
    "        print(\"Error: Unable to read the file.\")\n",
    "        return \"\"\n",
    "    \n",
    "def split_transcript(transcript: str, chunk_sizes: List[int], model: str = \"gpt-3.5-turbo-0613\") -> List[str]:\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    encoded_transcript = encoding.encode(transcript)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    for size in chunk_sizes:\n",
    "        end = start + size\n",
    "        chunk_tokens = encoded_transcript[start:end]\n",
    "        chunk_text = encoding.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def set_optimal_chunk_sizes(transcript_token, model_limit, chunk_min_length, max_token, prompt):\n",
    "    total_tokens = transcript_token + max_token + prompt\n",
    "    if total_tokens <= model_limit:\n",
    "        return None\n",
    "    \n",
    "    available_chunk_space = model_limit - max_token - prompt\n",
    "    fill_num = transcript_token // available_chunk_space\n",
    "    last_chunk_space = transcript_token % available_chunk_space\n",
    "\n",
    "    if last_chunk_space < chunk_min_length:\n",
    "        space_for_remaining_chunks = transcript_token - chunk_min_length\n",
    "        new_chunk_space = math.ceil(space_for_remaining_chunks / fill_num)\n",
    "        chunk_sizes = [new_chunk_space] * fill_num\n",
    "        chunk_sizes.append(chunk_min_length)\n",
    "        return chunk_sizes\n",
    "    else:\n",
    "        chunk_sizes = [available_chunk_space] * fill_num\n",
    "        chunk_sizes.append(last_chunk_space)\n",
    "        return chunk_sizes\n",
    "\n",
    "\n",
    "absolute_path = \"/Users/lucienlin/aiProjects/lucien-ai-meeting/data/test.txt\" \n",
    "transcript = read_txt_file(absolute_path)\n",
    "transcript_token =_count_tokens(transcript)\n",
    "model_limit = 4000\n",
    "chunk_min_length = 800\n",
    "max_token = 1200\n",
    "prompt = 200\n",
    "\n",
    "chunk_sizes = set_optimal_chunk_sizes(transcript_token, model_limit, chunk_min_length, max_token, prompt)\n",
    "print(chunk_sizes)\n",
    "chunk_list = []\n",
    "if not chunk_sizes:\n",
    "    chunk_list.append(transcript)\n",
    "else:\n",
    "    chunk_list = split_transcript(transcript, chunk_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二層 Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2606, 2190]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 計算第一層的每一個chunk當中的tokens\n",
    "chunk_token_list = []\n",
    "for chunk in chunk_list:\n",
    "    chunk_token_list.append(_count_tokens(chunk))\n",
    "chunk_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_layer1_result = [1100,1121]\n",
    "chunk_token_list = gpt_layer1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "啊啊gpt ['Tethers\\n資料的收入跟呈現的問題\\n這個是\\nAlisa\\n開的\\nAlisa\\n現在\\nOfficially\\n已經轉調了\\n是嗎?\\n不定時關心一下大家\\nTethers\\n資料呈現的問題\\n好\\n那就老樣子\\n就是\\nAlisa\\n你看不同意就抗辯\\n這題是還在進行中\\n是不是?\\n對\\n這個是預計下禮拜才會發問題\\n還沒有上\\n結發怎麼做都已經確定了\\n賣推波無法成功刪選\\n這個是什麼狀況?\\n這個也是正在處理\\n正在處理\\n18\\n號\\n那\\nAlisa\\n坐在這邊所以應該是\\nOK\\n啦\\n客戶版不會出現微頻的現象\\n這個我知道\\n這個就是\\n這個品牌就是在強制\\n這個是業務部拿來看客戶狀態的\\nGoogle\\nStudio\\n的那台貨\\n那這個是\\n那我覺得這個滿麻煩的\\n這就再看看是什麼\\n現在是誰?\\n我\\n好\\n我們等產銷會釐清資源以後再看\\n沒有請分位\\n這題是等著上線吧?\\n等著上線\\n這應該是等著上線\\n報告異常是?\\n這也在等著上線\\n所以是為什麼?\\n為什麼報告異常?\\n它有一個\\n它是兩個\\n非預期的\\n對\\n非預期的\\n你知道怎麼修的嗎?\\n它去判斷回應\\n它去判斷後面的回應\\n可是如果\\n主題關鍵是關聯性分數\\n我大概知道這個了\\n反正他們為了要判斷那個\\n有的時候\\n像其實這次歐文哥送出來有一個GDP的報告\\n主題是TEMP\\n以前是沒有辦法去\\n就黃牛嘛\\n對\\n你用TEMP\\n以前是沒有辦法Handle這一塊\\n那現在他們新增一些機制\\n可以從關鍵字去判斷說\\n你這份報告想要討論什麼\\n那看起來是那個機制出了點問題\\n所以導致查不出來報告\\n反正只要問題修掉就好\\n分數無法使用\\n那一樣就請Alisa\\n這題滿久的就請Alisa\\n看看這個可不可以接受\\n哇\\n任何關鍵字還是有網址\\n這個應該正好修吧\\n這是要等上線的問題\\n對\\n這可以再等上線\\nBD網頁建置需求\\n這個我知道這一題\\n那就是看\\n沒關係\\n這就先卡在這裡好了\\n因為這就是一個\\n用戶有需求需要評估人員\\n那我們這邊已經評估完畢了\\n那原則上這個就算是結束了\\n那擺一下看看還有沒有什麼後續\\n不然下禮拜就把它封掉了\\n頻道增加需求\\n那這個是\\nSteven成龍一個禮拜前開的\\n變成還在處理中\\n這個就好\\n反正就是加頻道\\n那這個\\n這大家都有在討論嘛\\n進行中\\n那現在看起來修到100不是什麼問題\\n對\\n配合定點\\n對\\n把它需求\\n這是Data\\nData就過了\\n那應該就是加就好了\\n信義標板這個實在\\n我覺得這種早晚也再討論一下\\n這個產品\\n這個會議叫做產品QA嘛\\n那信義網路品牌\\n還有以後等等的就是\\n我覺得其實上海這種市場都好\\n那只是\\n要各自建一個這種系統\\n那還都在\\n那也不可能都在這裡做檢討\\n然後怎麼區分\\n看起來現在有沒有統一的做法\\n就是有人用中華號什麼之類的\\n這中華號長得都不一樣\\n所以就這怎麼辦再來看\\n但是原則上這邊就不檢討\\n信義好了\\n提供資料需求\\n應該也是OK的\\n這Data\\n停機這個\\n這個就大家持續討論\\n停機維護的客戶名單\\n然後\\n編調\\n這個我知道\\n這是要影響內容嗎\\n不是\\n不是喔\\n好\\n我看一下\\n那這個\\n這個我覺得也不太像是\\n因為當初的設計\\n規劃到它一次開三個\\n主題的多維護中心\\n好\\n懂\\n我大概知道你的意思了\\n那我覺得這個\\n這個有一點灰色\\n我覺得站在使用者的角度\\n就會覺得說\\n就是應該要拍拍亮亮\\n不過沒有關係\\n那\\n就\\n這個是等著上來\\n好\\nOK\\n這個是亞里長妹\\n那這個是\\n我覺得這應該就是\\n你選擇套件就長那樣\\n好\\n那這有的解嗎\\n長高\\n長高\\n長高\\n好\\nOK\\n好的\\nOK\\n然後這個這題是那個\\n有一個\\n擷取資料API的需求\\n那事情火鍋是沒成功\\n那這個我再跟火鍋討論\\n關鍵是沒有拍單\\n這個是\\n興北他們的問題\\n可是他沒有回應\\n所以還擺在這裡\\n對\\n對\\n那這是BUG嗎\\n這個就不確定他們\\n想要得到的樣子是什麼\\n其實我一看\\n直覺覺得這應該不是BUG\\n應該就是他們在設定上\\n沒有拍\\n沒有設定到\\n拍他們預期想要的效果\\n然後就\\n我覺得應該是這樣\\n不過就\\n放心\\n是\\n好吧\\n沒辦法\\n那他們有權利拍\\n那回應這個再說\\n可是關鍵是好像\\n你們都沒有拍單\\n是不是\\n就是\\n比方說這篇文章\\n我先要我想要的\\n然後我排除我不想要的\\n那排除之後\\n明明那篇文章就有命中我\\n已經看到後面的那一串\\n可是\\n都沒有被排除\\n就是變成說\\n有時候\\n我要設定很長一串\\n太少被排掉\\n是\\n有時候是那個\\n邏輯\\n關鍵是邏輯\\n前後順序邏輯的問題\\n就是比如說有時候\\n中括號加個位置\\n然後\\n就是確定是都沒有\\n就是前面就很單純\\n前面就是括號\\n沒有我想要的內容而已\\n然後就經常要排除\\n不想要的\\n可是每次有時候\\n蠻常會遇到\\n我就算有排除了\\n但是那篇文章還是不見\\n就是如果是只有我遇到\\n可能我下次如果有遇到的話\\n再拆一個\\n因為我可以就是\\n段制的排除\\n比較有可能\\n因為不確定排到\\n排掉的是\\n哪一\\n是什麼樣的字\\n可是剛剛那個\\n他那個比�', '�\\n因為\\n他那個的\\n關鍵字都蠻明顯\\n也蠻常見的\\n就不確定他\\n想要格到的字\\n那個\\n等一下有發現了之後\\n我再麻煩\\n好\\n那就再麻煩\\n理事長碰到開個單\\n那這個\\n這個是\\n我知道大陸已經挖了\\n那現在有線索了嗎\\n還是\\n這個還沒\\n還沒\\n他\\n我知道\\n我知道大陸已經挖了\\n我知道\\n可是他那天跟\\n長輩做一些測試\\n然後\\n在資料上做一些測試\\n不像是資料的問題\\n可是還要再運程\\n可是程式裡面有動\\n程式裡面有動\\n程式裡面有動\\n也不像資料的問題\\n好吧\\n那就先這樣\\n那這個就\\n那想要問一下\\n那個小方那邊\\n就是\\n這題\\nPending在這邊這麼久\\n是可接受的嗎\\n我知道大陸已經沒什麼流量了\\n可是進去時\\n是\\n是\\n老實說我覺得有點\\n有點危險\\n就是要在某一個\\n拖這麼久\\n因為這個方式\\n我根本不知道\\n從什麼時候開始挖掘\\n我自己直覺是因為\\n不是\\n高山嗎\\n不是上面有一個很高嗎\\n對不起\\n第二個\\n不是\\n這個\\n但是他好奇怪\\n他進去的也不是我們的\\n這個就是手機的\\n我是覺得是上面的\\n資料掉了\\n你又不敢\\n他原本是哪些資料在裡面\\n其實他有點可疑\\n就是手機的問題\\n然後他回傳的\\n是\\n大概是要回傳哪段\\n我如果沒記錯\\n我雖然沒有記來\\n如果沒記錯的話\\n這邊是有兩段東西\\n一段是那個\\n最新文章\\n簽文者\\n然後另外一段是\\n就是其他分類的文章\\n他是首頁\\n就直接導到錯誤\\n我剛才是打這個三號\\n其實是去編的\\n很像\\n那你猜這是什麼\\n我都看了\\n但是就是\\n不知道哪一個\\n我還有一個很亮的建議\\n就是相反的\\n如果暫時短期內\\n解不掉的話\\n他講到這裡\\n之類的\\n就是首頁\\n進首頁就是進到\\n某一個文章列表\\n之類的\\n就\\n算是先暫時撐一撐\\n幫忙爭取時間\\n寫這個檔內\\n麻煩的一區\\n之類的\\n就是開始進入\\n因為看起來\\n就是首頁有問題\\n首頁會不會導到\\nERROR那個頁面\\n我踢一下\\n好\\n那頻道排除異常\\n這個\\n看影片\\n我的擔憂是\\n他的\\n頻道是\\n因為內容\\n和有逗號的關係造成\\n所以我還要再仔細看\\n確認一下什麼原因\\n因為\\n當初的這個資料\\n結構設計是從\\n日創\\n不是\\n從\\nERROR類\\n所以如果他是逗號的問題的話\\n可能會有點難理解\\n你說頻道名稱本人\\n對\\n也帶有逗號\\n好的\\n這個\\n你看我覺得\\n我好像上到這一條\\n我覺得IV是不知道\\n這是BUG還是這條\\nBUG\\n\\n這是BUG\\n對\\n所以那就麻煩\\n承接的\\n才可以確認\\n就幫我們分享\\n所以這是BUG\\n對\\n那我再\\n把電話\\n停掉\\n沒事\\n我以為IV很聰明\\n不是\\n你以為IV是個聰明人\\nIV是個聰明人嗎\\n對\\n是喔\\n這個火鍋就\\n體外化就賣\\n那每次會再\\nReassign\\n會給你一次\\n對啊這是我教他的\\n這樣我們就可以\\n隨時隨地知道\\n這個issue到底誰\\n還是誰在弄\\n我先Reassign給他\\n請他修\\n他修好再Reassign給我\\n好\\nOK\\n這樣就是\\n如果沒動\\n是\\n好的OK\\n那感謝\\n這邊今天就過完了\\n我們接下來就來看\\n大家有什麼要再交流的\\n我這邊要\\n問一個\\n就是\\nT-Mobile那個\\n還有斷線\\n因為他會直接\\n換掉現在\\nAPI的\\n資料\\n那API現在\\n還在測試\\n現在T-Mobile\\n設計測的就是\\n確認GDP\\n110\\n那其實測\\n我這樣子有測\\n其實\\n基本上下載是沒問題的\\n然後就\\n那個剩下\\n頻道標籤\\n因為我們反正也講到說\\n要在\\n要在\\n就是\\n資料要稍微調一下\\n不然就是給我\\n給我半天的時間\\n先切到頻道\\n可以這樣\\n測一下\\n他到底是有沒有這個\\n好\\n這樣\\n然後就\\n切回過來\\n先去明天\\n上班的時候\\n就切到\\n切到完\\n好\\n那\\n那今天下午可以先\\n切到\\n測到標籤嗎\\n可以啊\\n那我就\\n在群組\\n測試群組\\n好\\n因為\\n我怕上面\\n這條網站\\n我要卡不來了\\n萬一我要\\n再加\\n固標修正\\n你不用\\n你那邊有導播\\n我要開\\n切了\\n就這樣\\n好\\n這樣子\\n好\\n我們這邊可以\\n好\\n大家還有什麼\\n好那沒有的話\\n現在就這樣\\n然後我覺得\\n那個可能產銷會再討論一下\\n就是\\n定調\\n美國會的走向\\n不然的話\\n我覺得其實這個會議\\n因為以後Keys\\n其實參與的\\n必要性也很高\\n但是檢討說\\n既有產品的QA的問題\\n現在比較多\\n是在做這個\\n對不對\\n好的\\n感謝大家\\n等一下我們再討論一下\\n那個產銷會之類的\\n好\\n這樣子\\n小心']\n"
     ]
    }
   ],
   "source": [
    "def find_chunks_within_limit(model_limit, max_token, prompt, chunk_token_list):\n",
    "    chunk_token_list = [x + max_token + prompt for x in chunk_token_list]\n",
    "    \n",
    "    res = []\n",
    "    temp = []\n",
    "    \n",
    "    for idx, _ in enumerate(chunk_token_list):\n",
    "        temp.append(idx)\n",
    "        \n",
    "        if sum(chunk_token_list[temp[0]:idx+1]) > model_limit:\n",
    "            temp.pop()\n",
    "            res.append(temp)\n",
    "            temp = [idx]\n",
    "    \n",
    "    if temp:\n",
    "        res.append(temp)\n",
    "    \n",
    "    return res\n",
    "\n",
    "def simulate_gpt_layer2(chunk_list):\n",
    "    res = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk = 800\n",
    "        res.append(chunk)\n",
    "    return res\n",
    "\n",
    "def map_reduce(model_limit, max_token, prompt, chunk_token_list, chunk_list):\n",
    "    while True:\n",
    "        apple = find_chunks_within_limit(model_limit, max_token, prompt, chunk_token_list)\n",
    "        new_chunk_list = []\n",
    "        for indices in apple:\n",
    "            temp = []\n",
    "            for index in indices:\n",
    "                temp.append(chunk_list[index])\n",
    "            new_chunk_list.append(temp)\n",
    "        tres = []\n",
    "        for chunk in new_chunk_list:\n",
    "            ... #gpt\n",
    "            tres.append(...)\n",
    "        if tres == 1:\n",
    "            break\n",
    "    res = tres\n",
    "    return res # 回傳結果串列\n",
    "\n",
    "model_limit = 4000\n",
    "max_token = 500\n",
    "prompt = 200\n",
    "\n",
    "map_reduce(model_limit, max_token, prompt, chunk_token_list, chunk_list)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
